{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personalized-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#hide TF-related warnings in PySyft\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import syft as sy\n",
    "# from syft.frameworks.torch.fl import utils\n",
    "# from syft.workers.websocket_client import WebsocketClientWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-quest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "preceding-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the dataset with urllib2 to the current directory...\n",
      "The dataset was successfully downloaded\n",
      "Unzipping the dataset...\n",
      "Dataset successfully unzipped\n"
     ]
    }
   ],
   "source": [
    "#create a function for checking if the dataset does indeed exist\n",
    "def dataset_exists():\n",
    "    return (os.path.isfile('./data/eng-fra.txt') and\n",
    "    #check if all 18 files are indeed in the ./data/names/ directory\n",
    "    os.path.isdir('./data/names/') and\n",
    "    os.path.isfile('./data/names/Arabic.txt') and\n",
    "    os.path.isfile('./data/names/Chinese.txt') and\n",
    "    os.path.isfile('./data/names/Czech.txt') and\n",
    "    os.path.isfile('./data/names/Dutch.txt') and\n",
    "    os.path.isfile('./data/names/English.txt') and\n",
    "    os.path.isfile('./data/names/French.txt') and\n",
    "    os.path.isfile('./data/names/German.txt') and\n",
    "    os.path.isfile('./data/names/Greek.txt') and\n",
    "    os.path.isfile('./data/names/Irish.txt') and\n",
    "    os.path.isfile('./data/names/Italian.txt') and\n",
    "    os.path.isfile('./data/names/Japanese.txt') and\n",
    "    os.path.isfile('./data/names/Korean.txt') and\n",
    "    os.path.isfile('./data/names/Polish.txt') and\n",
    "    os.path.isfile('./data/names/Portuguese.txt') and\n",
    "    os.path.isfile('./data/names/Russian.txt') and\n",
    "    os.path.isfile('./data/names/Scottish.txt') and\n",
    "    os.path.isfile('./data/names/Spanish.txt') and\n",
    "    os.path.isfile('./data/names/Vietnamese.txt'))\n",
    "\n",
    "    \n",
    "#If the dataset does not exist, then proceed to download the dataset anew\n",
    "if not dataset_exists():\n",
    "    #If the dataset does not already exist, let's download the dataset directly from the URL where it is hosted\n",
    "    print('Downloading the dataset with urllib2 to the current directory...')\n",
    "    url = 'https://download.pytorch.org/tutorial/data.zip'\n",
    "    urllib.request.urlretrieve(url, './data.zip')\n",
    "    print(\"The dataset was successfully downloaded\")\n",
    "    print(\"Unzipping the dataset...\")\n",
    "    with ZipFile('./data.zip', 'r') as zipObj:\n",
    "       # Extract all the contents of the zip file in current directory\n",
    "       zipObj.extractall()\n",
    "    print(\"Dataset successfully unzipped\")\n",
    "else:\n",
    "    print(\"Not downloading the dataset because it was already downloaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "necessary-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the files in a certain path\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "#convert a string 's' in unicode format to ASCII format\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "criminal-ceremony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/Japanese.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Spanish.txt\n",
      "data/names/Polish.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/French.txt\n",
      "data/names/Irish.txt\n",
      "data/names/German.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/English.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Greek.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Czech.txt\n",
      "data/names/Russian.txt\n",
      "data/names/Portuguese.txt\n",
      "Amount of categories:18\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "#dictionary containing the nation as key and the names as values\n",
    "#Example: category_lines[\"italian\"] = [\"Abandonato\",\"Abatangelo\",\"Abatantuono\",...]\n",
    "category_lines = {}\n",
    "#List containing the different categories in the data\n",
    "all_categories = []\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    print(filename)\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines   \n",
    "    \n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"Amount of categories:\" + str(n_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "going-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    #Constructor is mandatory\n",
    "        def __init__(self, text, labels, transform=None):\n",
    "            self.data = text\n",
    "            self.targets = labels #categories\n",
    "            #self.to_torchtensor()\n",
    "            self.transform = transform\n",
    "        \n",
    "        def to_torchtensor(self):            \n",
    "            self.data = torch.from_numpy(self.text, requires_grad=True)\n",
    "            self.labels = torch.from_numpy(self.targets, requires_grad=True)\n",
    "        \n",
    "        def __len__(self):\n",
    "            #Mandatory\n",
    "            '''Returns:\n",
    "                    Length [int]: Length of Dataset/batches\n",
    "            '''\n",
    "            return len(self.data)\n",
    "    \n",
    "        def __getitem__(self, idx): \n",
    "            #Mandatory \n",
    "            \n",
    "            '''Returns:\n",
    "                     Data [Torch Tensor]: \n",
    "                     Target [ Torch Tensor]:\n",
    "            '''\n",
    "            sample = self.data[idx]\n",
    "            target = self.targets[idx]\n",
    "                    \n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "    \n",
    "            return sample,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list of arguments for our program. We will be needing most of them soon.\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 0.005\n",
    "        self.epochs = 10000\n",
    "        self.federate_after_n_batches = 15000\n",
    "        self.seed = 1\n",
    "        self.print_every = 200\n",
    "        self.plot_every = 100\n",
    "        self.use_cuda = False\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "included-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abukara', 'Adachi', 'Aida', 'Aihara', 'Aizawa', 'Ajibana', 'Akaike', 'Akamatsu', 'Akatsuka', 'Akechi', 'Akera', 'Akimoto', 'Akita', 'Akiyama', 'Akutagawa', 'Amagawa', 'Amaya', 'Amori', 'Anami']\n",
      "['Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese', 'Japanese']\n",
      "\n",
      " \n",
      " Amount of data points loaded: 20074\n"
     ]
    }
   ],
   "source": [
    "#Set of names(X)\n",
    "names_list = []\n",
    "#Set of labels (Y)\n",
    "category_list = []\n",
    "\n",
    "#Convert into a list with corresponding label.\n",
    "\n",
    "for nation, names in category_lines.items():\n",
    "    #iterate over every single name\n",
    "    for name in names:\n",
    "        names_list.append(name)      #input data point\n",
    "        category_list.append(nation) #label\n",
    "        \n",
    "#let's see if it was successfully loaded. Each data sample(X) should have its own corresponding category(Y)\n",
    "print(names_list[1:20])\n",
    "print(category_list[1:20])\n",
    "\n",
    "print(\"\\n \\n Amount of data points loaded: \" + str(len(names_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demanding-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Roijakkers', 'Romeijn', 'Romeijnders', 'Romeijnsen', 'Romijn', 'Romijnders', 'Romijnsen', 'Rompa', 'Rompa', 'Rompaeij']\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "categories_numerical = pd.factorize(category_list)[0]\n",
    "#Let's wrap our categories with a tensor, so that it can be loaded by LanguageDataset\n",
    "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
    "#Ready to be processed by torch.from_numpy in LanguageDataset\n",
    "categories_numpy = np.array(category_tensor)\n",
    "\n",
    "#Let's see a few resulting categories\n",
    "print(names_list[1200:1210])\n",
    "print(categories_numpy[1200:1210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "supposed-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abe\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(3, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "    \n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters) #Daniele: len(max_line_size) was len(line)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    #Daniele: add blank elements over here\n",
    "    return tensor    \n",
    "    \n",
    "    \n",
    "    \n",
    "def list_strings_to_list_tensors(names_list):\n",
    "    lines_tensors = []\n",
    "    for index, line in enumerate(names_list):\n",
    "        lineTensor = lineToTensor(line)\n",
    "        lineNumpy = lineTensor.numpy()\n",
    "        lines_tensors.append(lineNumpy)\n",
    "        \n",
    "    return(lines_tensors)\n",
    "\n",
    "lines_tensors = list_strings_to_list_tensors(names_list)\n",
    "\n",
    "print(names_list[0])\n",
    "print(lines_tensors[0])\n",
    "print(lines_tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "posted-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abe\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "torch.Size([19, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "max_line_size = max(len(x) for x in lines_tensors)\n",
    "\n",
    "def lineToTensorFillEmpty(line, max_line_size):\n",
    "    tensor = torch.zeros(max_line_size, 1, n_letters) #notice the difference between this method and the previous one\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "        \n",
    "        #Vectors with (0,0,.... ,0) are placed where there are no characters\n",
    "    return tensor\n",
    "\n",
    "def list_strings_to_list_tensors_fill_empty(names_list):\n",
    "    lines_tensors = []\n",
    "    for index, line in enumerate(names_list):\n",
    "        lineTensor = lineToTensorFillEmpty(line, max_line_size)\n",
    "        lines_tensors.append(lineTensor)\n",
    "    return(lines_tensors)\n",
    "\n",
    "lines_tensors = list_strings_to_list_tensors_fill_empty(names_list)\n",
    "\n",
    "#Let's take a look at what a word now looks like\n",
    "print(names_list[0])\n",
    "print(lines_tensors[0])\n",
    "print(lines_tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And finally, from a list, we can create a numpy array with all our word embeddings having the same shape:\n",
    "array_lines_tensors = np.stack(lines_tensors)\n",
    "#However, such operation introduces one extra dimension (look at the dimension with index=2 having size '1')\n",
    "print(array_lines_tensors.shape)\n",
    "#Because that dimension just has size 1, we can get rid of it with the following function call\n",
    "array_lines_proper_dimension = np.squeeze(array_lines_tensors, axis=2)\n",
    "print(array_lines_proper_dimension.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-desktop",
   "metadata": {},
   "source": [
    "# Data unbalancing and batch randomization:\n",
    "You may have noticed that our dataset is strongly unbalanced and contains a lot of data points in the \"russian.txt\" dataset. However, we would still like to take a random batch during our training procedure at every iteration. In order to prevent our neural network from classifying a data point as always belonging to the \"Russian\" category, we first pick a random category and then select a data point from that category. To do that, we construct a dictionary mapping a certain category to the corresponding starting index in the list of data points (e.g.: lines). Afterwards, we will take a datapoint starting from the starting_index identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "standing-gentleman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Japanese': 0, 'Dutch': 991, 'Spanish': 1288, 'Polish': 1586, 'Scottish': 1725, 'Arabic': 1825, 'French': 3825, 'Irish': 4102, 'German': 4334, 'Vietnamese': 5058, 'English': 5131, 'Korean': 8799, 'Greek': 8893, 'Chinese': 9096, 'Italian': 9364, 'Czech': 10073, 'Russian': 10592, 'Portuguese': 20000}\n"
     ]
    }
   ],
   "source": [
    "def find_start_index_per_category(category_list):\n",
    "    categories_start_index = {}\n",
    "    \n",
    "    #Initialize every category with an empty list\n",
    "    for category in all_categories:\n",
    "        categories_start_index[category] = []\n",
    "    \n",
    "    #Insert the start index of each category into the dictionary categories_start_index\n",
    "    #Example: \"Italian\" --> 203\n",
    "    #         \"Spanish\" --> 19776\n",
    "    last_category = None\n",
    "    i = 0\n",
    "    for name in names_list:\n",
    "        cur_category = category_list[i]\n",
    "        if(cur_category != last_category):\n",
    "            categories_start_index[cur_category] = i\n",
    "            last_category = cur_category\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    return(categories_start_index)\n",
    "\n",
    "categories_start_index = find_start_index_per_category(category_list)\n",
    "\n",
    "print(categories_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adequate-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    rand_value = random.randint(0, len(l) - 1)\n",
    "    return l[rand_value], rand_value\n",
    "\n",
    "\n",
    "def randomTrainingIndex():\n",
    "    category, rand_cat_index = randomChoice(all_categories) #cat = category, it's not a random animal\n",
    "    #rand_line_index is a relative index for a data point within the random category rand_cat_index\n",
    "    line, rand_line_index = randomChoice(category_lines[category])\n",
    "    category_start_index = categories_start_index[category]\n",
    "    absolute_index = category_start_index + rand_line_index\n",
    "    return(absolute_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-keeping",
   "metadata": {},
   "source": [
    "# 3. Step: Model - Recurrent Neural Network\n",
    "Hey, I must admit that was indeed a lot of data preprocessing and transformation, but it was well worth it! We have defined almost all the function we'll be needing during the training procedure and our data is ready to be fed into the neural network, which we're creating now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "short-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
      "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Two hidden layers, based on simple linear layers\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "#Let's instantiate the neural network already:\n",
    "n_hidden = 128\n",
    "#Instantiate RNN\n",
    "\n",
    "model = RNN(n_letters, n_hidden, n_categories)\n",
    "#The final softmax layer will produce a probability for each one of our 18 categories\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's define our workers. You can either use remote workers or virtual workers\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
    "charlie = sy.VirtualWorker(hook, id=\"charlie\") \n",
    "\n",
    "workers_virtual = [alice, bob]\n",
    "\n",
    "#If you have your workers operating remotely, like on Raspberry PIs\n",
    "#kwargs_websocket_alice = {\"host\": \"127.0.0.1\", \"hook\": hook}\n",
    "#alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket_alice)\n",
    "#kwargs_websocket_bob = {\"host\": \"127.0.0.1\", \"hook\": hook}\n",
    "#bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket_bob)\n",
    "#workers_virtual = [alice, bob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array_lines_proper_dimension = our data points(X)\n",
    "#categories_numpy = our labels (Y)\n",
    "langDataset =  LanguageDataset(array_lines_proper_dimension, categories_numpy)\n",
    "\n",
    "#assign the data points and the corresponding categories to workers.\n",
    "# PYSYFT\n",
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "            langDataset\n",
    "            .federate(workers_virtual),\n",
    "            batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-stanley",
   "metadata": {},
   "source": [
    "# 4. Step - Model Training!\n",
    "It's now time to train our Recurrent Neural Network based on the processed data. To do that, we need to define a few more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def fed_avg_every_n_iters(model_pointers, iter, federate_after_n_batches):\n",
    "        models_local = {}\n",
    "        \n",
    "        if(iter % args.federate_after_n_batches == 0):\n",
    "            for worker_name, model_pointer in model_pointers.items():\n",
    "                #need to assign the model to the worker it belongs to.\n",
    "                models_local[worker_name] = model_pointer.copy().get()\n",
    "            # PYSYFT\n",
    "            model_avg = utils.federated_avg(models_local)\n",
    "           \n",
    "            for worker in workers_virtual:\n",
    "                model_copied_avg = model_avg.copy()\n",
    "                model_ptr = model_copied_avg.send(worker) \n",
    "                model_pointers[worker.id] = model_ptr\n",
    "                \n",
    "        return(model_pointers)     \n",
    "\n",
    "def fw_bw_pass_model(model_pointers, line_single, category_single):\n",
    "    #get the right initialized model\n",
    "    model_ptr = model_pointers[line_single.location.id]   \n",
    "    line_reshaped = line_single.reshape(max_line_size, 1, len(all_letters))\n",
    "    #Firstly, initialize hidden layer\n",
    "    hidden_init = model_ptr.init_hidden() \n",
    "    #And now zero grad the model\n",
    "    model_ptr.zero_grad()\n",
    "    hidden_ptr = hidden_init.send(line_single.location)\n",
    "    amount_lines_non_zero = len(torch.nonzero(line_reshaped.copy().get()))\n",
    "    #now need to perform forward passes\n",
    "    for i in range(amount_lines_non_zero): \n",
    "        output, hidden_ptr = model_ptr(line_reshaped[i], hidden_ptr) \n",
    "    criterion = nn.NLLLoss()   \n",
    "    loss = criterion(output, category_single) \n",
    "    loss.backward()\n",
    "    \n",
    "    model_got = model_ptr.get() \n",
    "    \n",
    "    #Perform model weights' updates    \n",
    "    for param in model_got.parameters():\n",
    "        param.data.add_(-args.learning_rate, param.grad.data)\n",
    "        \n",
    "        \n",
    "    model_sent = model_got.send(line_single.location.id)\n",
    "    model_pointers[line_single.location.id] = model_sent\n",
    "    \n",
    "    return(model_pointers, loss, output)\n",
    "            \n",
    "  \n",
    "    \n",
    "def train_RNN(n_iters, print_every, plot_every, federate_after_n_batches, list_federated_train_loader):\n",
    "    current_loss = 0\n",
    "    all_losses = []    \n",
    "    \n",
    "    model_pointers = {}\n",
    "    \n",
    "    #Send the initialized model to every single worker just before the training procedure starts\n",
    "    for worker in workers_virtual:\n",
    "        model_copied = model.copy()\n",
    "        model_ptr = model_copied.send(worker) \n",
    "        model_pointers[worker.id] = model_ptr\n",
    "\n",
    "    #extract a random element from the list and perform training on it\n",
    "    for iter in range(1, n_iters + 1):        \n",
    "        random_index = randomTrainingIndex()\n",
    "        line_single, category_single = list_federated_train_loader[random_index]\n",
    "        #print(category_single.copy().get())\n",
    "        line_name = names_list[random_index]\n",
    "        model_pointers, loss, output = fw_bw_pass_model(model_pointers, line_single, category_single)\n",
    "        #model_pointers = fed_avg_every_n_iters(model_pointers, iter, args.federate_after_n_batches)\n",
    "        #Update the current loss\n",
    "        loss_got = loss.get().item() \n",
    "        current_loss += loss_got\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "             \n",
    "        if(iter % print_every == 0):\n",
    "            output_got = output.get()  #Without copy()\n",
    "            guess, guess_i = categoryFromOutput(output_got)\n",
    "            category = all_categories[category_single.copy().get().item()]\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss_got, line_name, guess, correct))\n",
    "    return(all_losses, model_pointers)def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-couple",
   "metadata": {},
   "source": [
    "In order for the defined randomization process to work, we need to wrap the data points and categories into a list, from that we're going to take a batch at a random index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This may take a few seconds to complete.\n",
    "print(\"Generating list of batches for the workers...\")\n",
    "list_federated_train_loader = list(federated_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "all_losses, model_pointers = train_RNN(args.epochs, args.print_every, args.plot_every, args.federate_after_n_batches, list_federated_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the loss we got during the training procedure\n",
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epochs (100s)')\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-lebanon",
   "metadata": {},
   "source": [
    "# 5. Step - Predict!\n",
    "Great! We have successfully created our two models for bob and alice in parallel using federated learning! I experimented with federated averaging of the two models, but it turned out that for a batch size of 1, as in the present case, the model loss was diverging. Let's try using our models for prediction now, shall we? This is the final reward for our endeavours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "center-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_line, worker, n_predictions=3):\n",
    "#     model = model.copy().get()\n",
    "    print('\\n> %s' % input_line)\n",
    "    model_remote = model.send(worker)\n",
    "    line_tensor = lineToTensor(input_line)\n",
    "    line_remote = line_tensor.copy().send(worker)\n",
    "    #line_tensor = lineToTensor(input_line)\n",
    "    #output = evaluate(model, line_remote)\n",
    "    # Get top N categories\n",
    "    hidden = model_remote.init_hidden()\n",
    "    hidden_remote = hidden.copy().send(worker)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i in range(line_remote.shape[0]):\n",
    "            output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
    "        \n",
    "    topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alice = model_pointers[\"alice\"].get()\n",
    "model_bob = model_pointers[\"bob\"].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_alice.copy(), \"Lu\", alice) \n",
    "predict(model_alice.copy(), \"Gadler\", alice) \n",
    "\n",
    "predict(model_bob.copy(), \"Lu\", alice) \n",
    "predict(model_bob.copy(), \"Gadler\", alice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
